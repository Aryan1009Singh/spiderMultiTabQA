{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import logging\n",
    "import mysql.connector\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up LLMs\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#gpt\n",
    "api_key_gpt=os.getenv('CHATGPT_API_KEY')\n",
    "client = OpenAI(\n",
    "  api_key=api_key_gpt,\n",
    ")\n",
    "\n",
    "#gemini\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY_FLASH\"])\n",
    "modelGemini=genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "#llama\n",
    "api_key_llama=os.getenv(\"LLAMA_API_KEY\")\n",
    "client = OpenAI(\n",
    "    api_key = api_key_llama,\n",
    "    base_url = \"https://api.llama-api.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResponse(prompt,llm):\n",
    "    # print(f\"User:\\n{prompt}\\n\\n\")\n",
    "    if llm=='gpt':\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "        )\n",
    "        response=chat_completion.choices[0].message.content\n",
    "        # print(f\"LLM:\\n{response}\\n\\n\")\n",
    "        return response\n",
    "    elif llm=='gemini':\n",
    "        time.sleep(3)\n",
    "        response = modelGemini.generate_content(prompt)\n",
    "        # print(f\"LLM:\\n{response.text}\\n\\n\")\n",
    "        return response.text\n",
    "    elif llm=='llama':\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-13b-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        response=response.choices[0].message.content\n",
    "        # print(f\"LLM:\\n{response}\\n\\n\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute_query(database_path, query):\n",
    "    try:\n",
    "        conn = sqlite3.connect(database_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        with open('./incorrectGeminiLog.txt', 'a') as f:\n",
    "            f.write(f\"\\nError that occured:\\n{e}\\n\\n\")\n",
    "        return None\n",
    "\n",
    "def get_first_row_with_columns(database_path, table_name):\n",
    "    try:\n",
    "        conn = sqlite3.connect(database_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        columns = [info[1] for info in cursor.fetchall()]\n",
    "        \n",
    "        # Get the first row\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT 1\")\n",
    "        first_row = cursor.fetchone()\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return columns, first_row\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        with open('./incorrectGeminiLog.txt', 'a') as f:\n",
    "            f.write(f\"\\nError that occurred:\\n{e}\\n\\n\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getDbSchemaMapping(dbFolderPath):\n",
    "    count = 0\n",
    "    schema_array = {}\n",
    "    for folder in os.listdir(dbFolderPath):\n",
    "        count += 1\n",
    "        folder_path = os.path.join(dbFolderPath, folder)\n",
    "        if os.path.exists(folder_path):\n",
    "            json_data = None\n",
    "            table_info = []\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith('.json'):\n",
    "                    json_file_path = os.path.join(folder_path, file_name)\n",
    "                    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "                        try:\n",
    "                            json_data = json.load(file)\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error decoding JSON in file {json_file_path}: {e}\")\n",
    "                            continue\n",
    "            \n",
    "            db_file_path = os.path.join(folder_path, f\"{folder}.sqlite\")\n",
    "            if json_data:\n",
    "                if 'tables' in json_data:\n",
    "                    final_table_info = {}\n",
    "                    for table in json_data['tables']:\n",
    "                        table_name = table['name']\n",
    "                        columns, first_row = get_first_row_with_columns(db_file_path, table_name)\n",
    "                        table_info.append((table_name, columns, first_row))\n",
    "                    \n",
    "                        \n",
    "                    schema_array[folder] = {\n",
    "                        \"schema\": json_data,\n",
    "                        \"table_info\": table_info\n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"'tables' key not found in JSON data for folder: {folder_path}\")\n",
    "                    schema_array[folder] = {\n",
    "                        \"schema\": json_data,\n",
    "                        \"table_info\": table_info\n",
    "                    }\n",
    "            else:\n",
    "                print(f\"JSON file not found for folder: {folder_path}\")\n",
    "    print(count)\n",
    "    return schema_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tableJsonPath,cleanDataPath):\n",
    "\n",
    "    with open(tableJsonPath,\"r\") as f:\n",
    "        tables_data=json.load(f)\n",
    "\n",
    "    database = {}\n",
    "\n",
    "    with open(cleanDataPath, 'r') as f:\n",
    "\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            db_id = data.get('db_id')\n",
    "            query = data.get('query')\n",
    "            question = data.get('question')\n",
    "            query_toks=data.get('query_toks')\n",
    "\n",
    "            word_freq={}        \n",
    "            for item in tables_data:\n",
    "                if item['db_id']==db_id:\n",
    "                    for table_name in item[\"table_names_original\"]:\n",
    "                        word_freq[table_name.lower()]=1\n",
    "\n",
    "            if db_id not in database:\n",
    "                database[db_id] = {'query': [], 'question': [], 'query_toks': [], 'tables': []}\n",
    "\n",
    "            interim_map={}\n",
    "            table_list=[]\n",
    "            for query_tok in query_toks:\n",
    "                if query_tok.lower() in word_freq:\n",
    "                    if query_tok.lower() not in interim_map:\n",
    "                        interim_map[query_tok.lower()]=1\n",
    "                        table_list.append(query_tok.lower())\n",
    "            \n",
    "            database[db_id]['query'].append(query)\n",
    "            database[db_id]['question'].append(question)\n",
    "            database[db_id]['query_toks'].append(query_toks)\n",
    "            database[db_id]['tables'].append(table_list)\n",
    "            \n",
    "\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_array = getDbSchemaMapping('F:/OneDrive/Desktop/Study/NLP_ResearchProject/Project/spider/database')\n",
    "databases = get_data('F:/OneDrive/Desktop/Study/NLP_ResearchProject/Project/spider/tables.json','F:/OneDrive/Desktop/Study/NLP_ResearchProject/Project/spider/train_spider_main_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_dataset = []\n",
    "easy_dataset = []\n",
    "\n",
    "for folder, data in schema_array.items():\n",
    "    schema = data['schema']\n",
    "    table_info = data['table_info']\n",
    "    length = len(table_info)\n",
    "\n",
    "    if length >= 6:\n",
    "        hard_dataset.append(folder)\n",
    "    else:\n",
    "        easy_dataset.append(folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise cross prompt variiables (DANGER: DO NOT PRESS)\n",
    "totalQueries=0\n",
    "correctAns=0\n",
    "notCorrectAns=0\n",
    "file_path = 'F:/OneDrive/Desktop/Study/NLP_ResearchProject/Project/incorrectGeminiLogTableExtraction.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    pass\n",
    "file_path = 'F:/OneDrive/Desktop/Study/NLP_ResearchProject/Project/geminiLogTableExtraction.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('database_table_extract_questions.json', 'r') as json_file:\n",
    "    database_table_questions = json.load(json_file)\n",
    "\n",
    "with open('database_table_extract.json', 'r') as json_file:\n",
    "    database_table_description = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder, tables in database_table_description.items():\n",
    "    # Create a new dictionary with keys converted to lower case\n",
    "    tables_lower = {table.lower(): table_description for table, table_description in tables.items()}\n",
    "    # Replace the original dictionary with the modified one\n",
    "    database_table_description[folder] = tables_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder, tables in database_table_questions.items():\n",
    "    if len(database_table_description[folder]) <= 5:\n",
    "        continue\n",
    "    print(f\"Folder is {folder}\")\n",
    "    print(f\"Number of Tables are {len(database_table_description[folder])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised learning chinook_1 college_1\n",
    "model_used = 'gemini'\n",
    "threshold = 0.5\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "db_table_extraction_list = {}\n",
    "total_num_correct = 0\n",
    "total_num_incorrect = 0\n",
    "database_selected_tables = {}\n",
    "\n",
    "for folder, tables in database_table_questions.items():\n",
    "    \n",
    "    \n",
    "    tables_descriptions = database_table_description[folder]\n",
    "    # if len(database_table_description[folder]) <= 5:\n",
    "    #     continue\n",
    "    print(f\"Folder: {folder}\")\n",
    "    num_questions = len(databases[folder]['question'])\n",
    "    num_correct = 0\n",
    "    num_incorrect = 0\n",
    "    tables_and_questions = {}\n",
    "    for i in range(num_questions):\n",
    "        question = databases[folder]['question'][i]\n",
    "        question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
    "        table_similarities = {}\n",
    "\n",
    "        for table_name, questions in tables.items():\n",
    "            if table_name not in tables_descriptions:\n",
    "                continue  # Skip this iteration if the table_name key doesn't exist in tables_descriptions\n",
    "            table_description = tables_descriptions[table_name]\n",
    "            similarity_val = 0\n",
    "            descriptor_embedding = embedder.encode(table_description, convert_to_tensor=True)\n",
    "            similarity_val_desc = util.pytorch_cos_sim(question_embedding, descriptor_embedding)\n",
    "            similarity_val2 = similarity_val_desc.item()\n",
    "            for question2 in questions:\n",
    "                question2_embedding = embedder.encode(question2, convert_to_tensor=True)\n",
    "                similarity = util.pytorch_cos_sim(question_embedding, question2_embedding)\n",
    "                similarity_val = max(similarity_val, similarity.item())\n",
    "            similarity_val = similarity_val + similarity_val2          \n",
    "            \n",
    "            table_similarities[table_name.lower()] = similarity_val\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Table Similarities: {table_similarities}\")\n",
    "        \n",
    "\n",
    "        num_elements = len(table_similarities)\n",
    "        \n",
    "\n",
    "        \n",
    "        try:\n",
    "            average_score = sum(table_similarities.values()) / len(table_similarities)\n",
    "        except ZeroDivisionError:\n",
    "            average_score = 0  # or any other default value or action\n",
    "        selected_tables = []\n",
    "        \n",
    "        sorted_tables = sorted(table_similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "        cnt = 0\n",
    "        prev = 0\n",
    "        thres = 50\n",
    "        flg = False\n",
    "        flg2 = True\n",
    "        for table_name, score in sorted_tables:  # Corrected this line\n",
    "            cnt = cnt + 1\n",
    "            if cnt > 1 and flg2 == True:\n",
    "                thres = min(thres, (prev/score))\n",
    "\n",
    "            if cnt > 4:\n",
    "                flg2 = False\n",
    "                if not flg:\n",
    "                    thres = prev/score\n",
    "                    flg = True\n",
    "                else:\n",
    "                    if (prev/score) > thres:\n",
    "                        break\n",
    "                    thres = (prev/score)\n",
    "            selected_tables.append(table_name)\n",
    "            prev = score\n",
    "        \n",
    "            \n",
    "        print(f\"Selected Tables: {selected_tables}\")\n",
    "        tables_and_questions[i] = selected_tables\n",
    "        \n",
    "        table_list = databases[folder]['tables'][i]\n",
    "        query = databases[folder]['query'][i]\n",
    "        print(f\"Table List: {table_list}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Check if all tables in table_list are present in selected_tables\n",
    "        if all(table in selected_tables for table in table_list):\n",
    "            print(\"Correct\")\n",
    "            num_correct += 1\n",
    "            total_num_correct += 1\n",
    "        else:\n",
    "            print(\"Incorrect\")\n",
    "            num_incorrect += 1\n",
    "            total_num_incorrect += 1\n",
    "        print(\"\\n\")\n",
    "    database_selected_tables[folder] = tables_and_questions\n",
    "    \n",
    "    percent = (num_correct / num_questions) * 100\n",
    "    print(f\"{folder} has this much accuracy {percent}\")\n",
    "    db_table_extraction_list[folder] = {\n",
    "        \"Accuracy\": percent,\n",
    "        \"Correct\": num_correct,\n",
    "        \"Incorrect\": num_incorrect\n",
    "    }\n",
    "    print(\"\\n\")\n",
    "\n",
    "try:\n",
    "    total_percent = (total_num_correct / (total_num_correct + total_num_incorrect)) * 100\n",
    "except ZeroDivisionError:\n",
    "    total_percent = 0\n",
    "\n",
    "print(f\"Total Percent Accuracy: {total_percent}\")\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "# Select the tables with similarity scores above the average score / 75% quartile\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(databases['icfp_1']['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tables = {}\n",
    "for folder, accuracy in db_table_extraction_list.items():\n",
    "    print(f\"Folder: {folder}\")\n",
    "    print(f\"Accuracy: {accuracy['Accuracy']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('selected_tables.json', 'w') as json_file:\n",
    "    json.dump(database_selected_tables, json_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "overall_metric_values = {}\n",
    "\n",
    "for folder, val in database_selected_tables.items():\n",
    "    if folder in easy_dataset:\n",
    "        continue\n",
    "    database_true_positives = 0\n",
    "    database_false_positives = 0\n",
    "    database_false_negatives = 0\n",
    "    for i, tables in val.items():\n",
    "        tables_selected = set(tables)  # Assuming tables is a list, convert it to a set for efficient operations\n",
    "        original_table = set(databases[folder]['tables'][i])  # Similarly, ensure original_table is a set\n",
    "\n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        database_true_positives += len(tables_selected.intersection(original_table))\n",
    "        database_false_positives += len(tables_selected - original_table)\n",
    "        database_false_negatives += len(original_table - tables_selected)\n",
    "    database_recall = database_true_positives / (database_true_positives + database_false_negatives) if (database_true_positives + database_false_negatives) > 0 else 0\n",
    "    database_precision = database_true_positives / (database_true_positives + database_false_positives) if (database_true_positives + database_false_positives) > 0 else 0\n",
    "    dic = {\n",
    "    \"recall\": database_recall ,\n",
    "    \"precision\": database_precision,\n",
    "    \"accuracy\": db_table_extraction_list[folder][\"Accuracy\"]\n",
    "}\n",
    "    overall_metric_values[folder] = dic\n",
    "\n",
    "\n",
    "    true_positives += database_true_positives\n",
    "    false_negatives += database_false_negatives\n",
    "    false_positives += database_false_positives\n",
    "\n",
    "\n",
    "# Calculate recall and precision\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "\n",
    "print(f\"Recall: {recall * 100}\")\n",
    "print(f\"Precision: {precision * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./strategy_3.json\", \"w\") as f:\n",
    "    json.dump(overall_metric_values, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for folder, values in overall_metric_values.items():\n",
    "    print(f\"Folder: {folder}\")\n",
    "    if isinstance(values['precision'], list):\n",
    "        precision = sum([value * 100 for value in values['precision']]) / len(values['precision'])\n",
    "    else:\n",
    "        precision = values['precision'] * 100\n",
    "    print(f\"Precision: {precision}%\")\n",
    "    print(f\"Recall: {values['recall'] * 100}%\")\n",
    "    print(f\"Accuracy: {values['accuracy']}%\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_metric_values.json\", \"w\") as json_file:\n",
    "    json.dump(overall_metric_values, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_table_extraction_percent_accuracy = {}\n",
    "easy_table_extraction_percent_accuracy = {}\n",
    "\n",
    "for folder, accuracy in db_table_extraction_list.items():\n",
    "    if folder in hard_dataset:\n",
    "        hard_table_extraction_percent_accuracy[folder] = accuracy\n",
    "    else:\n",
    "        easy_table_extraction_percent_accuracy[folder] = accuracy\n",
    "\n",
    "with open(\"hard_table_extraction_percent_accuracy.json\", \"w\") as f:\n",
    "    json.dump(hard_table_extraction_percent_accuracy, f, indent=4)\n",
    "with open(\"easy_table_extraction_percent_accuracy.json\", \"w\") as f:\n",
    "    json.dump(easy_table_extraction_percent_accuracy, f, indent=4)\n",
    "\n",
    "hard_num_correct = 0\n",
    "hard_num_incorrect = 0\n",
    "hard_percent = 0\n",
    "\n",
    "for folder, accuracy in hard_table_extraction_percent_accuracy.items():\n",
    "    hard_num_correct += accuracy['Correct']\n",
    "    hard_num_incorrect += accuracy['Incorrect']\n",
    "hard_percent = (hard_num_correct / (hard_num_correct + hard_num_incorrect)) * 100\n",
    "\n",
    "easy_num_correct = 0\n",
    "easy_num_incorrect = 0\n",
    "easy_percent = 0\n",
    "\n",
    "for folder, accuracy in easy_table_extraction_percent_accuracy.items():\n",
    "    easy_num_correct += accuracy['Correct']\n",
    "    easy_num_incorrect += accuracy['Incorrect']\n",
    "easy_percent = (easy_num_correct / (easy_num_correct + easy_num_incorrect)) * 100\n",
    "hard_size = len(hard_table_extraction_percent_accuracy)\n",
    "easy_size = len(easy_table_extraction_percent_accuracy)\n",
    "percent = {\n",
    "    \"Hard\": hard_percent,\n",
    "    \"Easy\": easy_percent,\n",
    "    \"total\": total_percent,\n",
    "    \"Hard Size\": hard_size,\n",
    "    \"Easy Size\": easy_size,\n",
    "    \"hard_num_correct\": hard_num_correct,\n",
    "    \"hard_num_incorrect\": hard_num_incorrect,\n",
    "    \"easy_num_correct\": easy_num_correct,\n",
    "    \"easy_num_incorrect\": easy_num_incorrect\n",
    "}\n",
    "\n",
    "with open(\"table_extraction_percent_question.json\", \"w\") as f:\n",
    "    json.dump(percent, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
